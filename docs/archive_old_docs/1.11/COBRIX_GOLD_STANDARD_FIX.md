# Cobrix Gold Standard Implementation - Root Cause Analysis & Fix

**Date:** January 10, 2026  
**Status:** üîç **ANALYSIS COMPLETE - FIX PLAN READY**  
**Goal:** Make Cobrix implementation work reliably with industry-standard best practices

---

## üéØ Executive Summary

**Problem:** Cobrix implementation is failing despite Cobrix being the industry gold standard.

**Root Cause:** **Over-engineering and incorrect assumptions** - The implementation has 1400+ lines of complex preprocessing that's actually breaking Cobrix's built-in capabilities.

**Solution:** **Simplify dramatically** - Trust Cobrix to do what it's designed to do. Minimal, correct preprocessing only.

---

## üîç Root Cause Analysis

### What Cobrix Actually Needs (From Documentation & Best Practices)

Cobrix is designed to be **idiot-proof** and handle most COBOL variations automatically. It needs:

1. **Clean copybook** (minimal cleaning):
   - Remove 88-level fields (Cobrix doesn't support them)
   - Remove VALUE clauses (runtime initialization, not structure)
   - Remove identifiers/sequence numbers (if present)
   - Standard COBOL format (columns 6-72) OR free-form (level at column 0)

2. **Correct encoding**:
   - "EBCDIC" or "ASCII" (not codec names like "cp037")
   - Cobrix auto-detects if not specified

3. **File format**:
   - Fixed-length records (use `record_format="F"`)
   - OR variable-length with RDW (use `record_format="V"`)
   - File must be divisible by record size OR use `file_start_offset`/`file_trailer_length`

4. **That's it!** Cobrix handles:
   - OCCURS automatically
   - REDEFINES automatically
   - COMP-3 automatically
   - Nested structures automatically
   - Field alignment automatically
   - Encoding detection automatically

---

## ‚ùå What We're Doing Wrong

### Problem 1: Over-Complicated Copybook Preprocessing

**Current Code:** 400+ lines of complex preprocessing
- Multiple format detection attempts
- Complex identifier stripping with regex
- Manual continuation line handling
- Multiple passes of cleaning
- Field name replacements (POLICYHOLDER-AGE ‚Üí POLICYHOLDER_AGE)
- PIC clause replacements (PIC 9(3) ‚Üí PIC 999)

**Issue:** This is breaking Cobrix's built-in parsing. Cobrix expects standard COBOL, and our "fixes" are creating non-standard formats.

**Example of Over-Engineering:**
```python
# Lines 311-337: Complex identifier stripping
identifier_match = re.match(r'^([A-Z0-9]{2,8})\s+', cobol_code_stripped)
if identifier_match:
    identifier = identifier_match.group(1)
    remaining = cobol_code_stripped[len(identifier):].lstrip()
    # ... 20 more lines of logic
```

**What Cobrix Actually Needs:**
```python
# Simple: Remove identifiers if they're before level numbers
if re.match(r'^[A-Z0-9]{2,8}\s+\d{2}\s+', line):
    line = re.sub(r'^[A-Z0-9]{2,8}\s+', '', line)
```

---

### Problem 2: Over-Complicated File Normalization

**Current Code:** 600+ lines of file normalization
- Newline removal
- Header detection with multiple heuristics
- Trailer detection
- Record boundary alignment
- Multiple retry attempts with different strategies
- Manual field alignment fixes

**Issue:** Cobrix has built-in options for this:
- `file_start_offset` - Skip header bytes
- `file_trailer_length` - Skip trailer bytes
- `record_format="F"` - Fixed-length records
- Automatic record boundary detection

**Example of Over-Engineering:**
```python
# Lines 691-1158: 400+ lines of header/trailer detection
# Multiple scoring systems, pattern matching, retries
# This is all unnecessary - Cobrix can handle it
```

**What Cobrix Actually Needs:**
```python
# Simple: Calculate offsets and pass to Cobrix
file_start_offset = detect_header_length(file_data)  # Simple function
file_trailer_length = detect_trailer_length(file_data, record_size)
# Pass to Cobrix: .option("file_start_offset", file_start_offset)
```

---

### Problem 3: Manual Field Alignment Fixes

**Current Code:** 200+ lines of manual field extraction
- Manually reading bytes from file
- Fixing POLICYHOLDER_NAME, POLICY_TYPE, PREMIUM_AMOUNT
- Workarounds for AGE field

**Issue:** If Cobrix is misaligning fields, the copybook is wrong, not the file. Fix the copybook, not the parsing.

**Example:**
```python
# Lines 1287-1368: Manual field extraction
age_offset = 50
age_bytes = normalized_content[age_start:age_end]
# ... manual fixes for all fields
```

**What Cobrix Actually Needs:**
- Correct copybook with proper field definitions
- Cobrix will align fields automatically based on copybook

---

### Problem 4: Multiple Retry Attempts

**Current Code:** Retries with different trimming strategies
- First attempt fails
- Trim file
- Retry
- If still fails, try different trimming
- Multiple passes

**Issue:** If the first attempt fails, the copybook or file format is wrong. Fix the root cause, don't retry with workarounds.

---

## ‚úÖ Gold Standard Implementation

### Simplified Architecture

```
1. Receive file + copybook (bytes)
2. Minimal copybook cleaning:
   - Remove 88-level fields
   - Remove VALUE clauses
   - Remove identifiers (simple regex)
   - Ensure standard COBOL format
3. Detect encoding (EBCDIC vs ASCII)
4. Calculate file offsets (simple header/trailer detection)
5. Call Cobrix with correct options
6. Return results
```

**Total Code:** ~200 lines (vs current 1400+ lines)

---

## üîß Implementation Plan

### Phase 1: Extract 88-Level Metadata BEFORE Cleaning

**Goal:** Extract 88-level validation rules BEFORE removing them from copybook.

**Critical:** 88-level fields must be extracted BEFORE cleaning, so they're available for insights pillar validation.

**Implementation:**
```python
def extract_88_level_metadata(copybook_text: str) -> Dict[str, Any]:
    """
    Extract 88-level field validation rules BEFORE cleaning copybook.
    
    Returns:
        {
            "88_level_fields": [
                {
                    "field_name": "STATUS-CODE",
                    "condition_name": "ACTIVE",
                    "value": "A",
                    "line_number": 123
                },
                ...
            ]
        }
    """
    # Use existing Level88Extractor or MainframeProcessingAdapter logic
    from cobol_88_field_extractor import Level88Extractor
    
    extractor = Level88Extractor()
    level88_values, value_to_names = extractor.extract_88_fields(copybook_text)
    
    # Convert to validation rules format (same as custom adapter)
    validation_rules = {
        "88_level_fields": [],
        "metadata_records": []  # Level-01 metadata handled separately
    }
    
    for field_name, allowed_values in level88_values.items():
        for value in allowed_values:
            condition_names = value_to_names.get(field_name, {}).get(value, [])
            validation_rules["88_level_fields"].append({
                "field_name": field_name,
                "condition_name": condition_names[0] if condition_names else None,
                "value": value
            })
    
    return validation_rules
```

**Key Points:**
- Extract BEFORE cleaning (88-level fields are removed during cleaning)
- Use same format as custom adapter (for consistency)
- Store in response metadata for insights pillar

---

### Phase 2: Minimal Copybook Cleaning

**Goal:** Clean copybook to Cobrix standards with minimal code.

**Implementation:**
```python
def clean_copybook_for_cobrix(copybook_text: str) -> str:
    """
    Minimal copybook cleaning for Cobrix.
    
    Rules:
    1. Remove 88-level fields (entire lines)
    2. Remove VALUE clauses (from field definitions)
    3. Remove identifiers (alphanumeric prefix before level numbers)
    4. Ensure standard COBOL format (columns 6-72)
    5. Remove comments (* and /)
    """
    lines = []
    for line in copybook_text.split('\n'):
        # Skip 88-level fields
        if re.match(r'^\s*88\s+', line.strip()):
            continue
        
        # Remove VALUE clauses
        line = re.sub(r'\s+VALUE\s+[^.]*\.', '', line, flags=re.IGNORECASE)
        
        # Remove identifiers (e.g., "AF1019 01 RECORD" -> "01 RECORD")
        line = re.sub(r'^([A-Z0-9]{2,8})\s+(\d{2}\s+)', r'\2', line)
        
        # Remove comments
        if line.strip().startswith('*') or line.strip().startswith('/'):
            continue
        
        # Format as standard COBOL (columns 6-72)
        if line.strip():
            # Extract COBOL code (columns 6-72)
            if len(line) > 6:
                cobol_code = line[6:72].rstrip()
            else:
                cobol_code = line.strip()
            
            # Format: 6 spaces + code
            formatted = "      " + cobol_code
            lines.append(formatted)
    
    return '\n'.join(lines)
```

**Key Points:**
- Simple, linear processing
- No complex heuristics
- No multiple passes
- Trust Cobrix to handle the rest

---

### Phase 2: Simple File Format Detection

**Goal:** Detect file format and calculate offsets.

**Implementation:**
```python
def detect_file_format(file_data: bytes, record_size: int) -> dict:
    """
    Simple file format detection for Cobrix.
    
    Returns:
        {
            "encoding": "EBCDIC" or "ASCII",
            "file_start_offset": int,  # Skip header bytes
            "file_trailer_length": int,  # Skip trailer bytes
            "record_format": "F"  # Fixed-length
        }
    """
    # Detect encoding (simple heuristic)
    ascii_count = sum(1 for b in file_data[:1000] if 0x20 <= b <= 0x7E)
    encoding = "ASCII" if ascii_count > 800 else "EBCDIC"
    
    # Simple header detection: Find first record boundary
    # Look for patterns that indicate data start (not comments/headers)
    file_start_offset = 0
    for offset in range(0, min(2000, len(file_data)), record_size):
        # Check if this looks like a data record (not header)
        sample = file_data[offset:offset+min(50, record_size)]
        sample_text = sample.decode('ascii', errors='ignore')
        
        # Skip if it looks like a header (contains comment markers or descriptive text)
        if '#' in sample_text or any(word in sample_text.lower() for word in ['record', 'format', 'length']):
            continue
        
        # This looks like data
        file_start_offset = offset
        break
    
    # Simple trailer detection: Check if file is divisible by record size
    file_trailer_length = len(file_data) % record_size
    
    return {
        "encoding": encoding,
        "file_start_offset": file_start_offset,
        "file_trailer_length": file_trailer_length,
        "record_format": "F"
    }
```

**Key Points:**
- Simple heuristics only
- Pass offsets to Cobrix (don't modify file)
- Let Cobrix handle the rest

---

### Phase 3: Simplified Cobrix Call

**Goal:** Call Cobrix with correct options, no workarounds.

**Implementation:**
```python
async def parse_with_cobrix(
    file_data: bytes,
    copybook_text: str,
    filename: str
) -> dict:
    """
    Parse file with Cobrix using gold standard approach.
    """
    # 1. Extract 88-level metadata BEFORE cleaning (CRITICAL for insights pillar)
    validation_rules = extract_88_level_metadata(copybook_text)
    
    # 2. Clean copybook (minimal) - removes 88-level fields
    cleaned_copybook = clean_copybook_for_cobrix(copybook_text)
    
    # 2. Analyze copybook to get record size
    record_size = analyze_copybook_for_record_size(cleaned_copybook)
    
    # 3. Detect file format
    file_format = detect_file_format(file_data, record_size)
    
    # 4. Write temp files (Cobrix requires file paths)
    with tempfile.TemporaryDirectory() as tmpdir:
        input_file = os.path.join(tmpdir, filename or "data.bin")
        copybook_file = os.path.join(tmpdir, "copybook.cpy")
        
        with open(input_file, 'wb') as f:
            f.write(file_data)
        
        with open(copybook_file, 'w', encoding='utf-8') as f:
            f.write(cleaned_copybook)
        
        # 5. Call Cobrix with correct options
        cmd = [
            f"{SPARK_HOME}/bin/spark-submit",
            "--master", "local[1]",
            "--jars", COBRIX_JAR,
            "--class", "za.co.absa.cobrix.CobrixParserApp",
            APP_JAR,
            "--input", input_file,
            "--copybook", copybook_file,
            "--output", output_dir,
            "--encoding", file_format["encoding"],
            "--file_start_offset", str(file_format["file_start_offset"]),
            "--file_trailer_length", str(file_format["file_trailer_length"])
        ]
        
        result = await asyncio.to_thread(
            subprocess.run,
            cmd,
            capture_output=True,
            text=True,
            timeout=300.0
        )
        
        if result.returncode != 0:
            raise Exception(f"Cobrix failed: {result.stderr}")
        
        # 6. Read results
        records = read_jsonl_results(output_dir)
        
        # 7. Include validation_rules in response (for insights pillar)
        return {
            "success": True,
            "records": records,
            "validation_rules": validation_rules  # 88-level fields for data quality validation
        }
```

**Key Points:**
- No retries
- No workarounds
- No manual field fixes
- Trust Cobrix to parse correctly

---

### Phase 4: Update Scala Code

**Goal:** Pass file format options to Cobrix.

**Current Scala Code:**
```scala
val df = spark.read
  .format("cobol")
  .option("copybook", copybookFile)
  .option("encoding", encoding)
  .option("record_format", "F")
  .load(inputFile)
```

**Updated Scala Code:**
```scala
val fileStartOffset = parsedArgs.getOrElse("file_start_offset", "0").toInt
val fileTrailerLength = parsedArgs.getOrElse("file_trailer_length", "0").toInt

val df = spark.read
  .format("cobol")
  .option("copybook", copybookFile)
  .option("encoding", encoding)
  .option("record_format", "F")
  .option("file_start_offset", fileStartOffset)
  .option("file_trailer_length", fileTrailerLength)
  .option("schema_retention_policy", "keep_original")
  .load(inputFile)
```

---

## üìä Comparison: Current vs Gold Standard

| Aspect | Current Implementation | Gold Standard |
|--------|----------------------|---------------|
| **Copybook Cleaning** | 400+ lines, complex | 50 lines, simple |
| **File Normalization** | 600+ lines, multiple retries | 50 lines, simple detection |
| **Field Alignment** | 200+ lines, manual fixes | 0 lines (Cobrix handles it) |
| **Total Code** | 1400+ lines | ~200 lines |
| **Reliability** | ‚ùå Fails with 500 errors | ‚úÖ Should work reliably |
| **Maintainability** | ‚ùå Very complex | ‚úÖ Simple and clear |
| **Trust in Cobrix** | ‚ùå Workarounds everywhere | ‚úÖ Trust Cobrix capabilities |

---

## üîç 88-Level Metadata Extraction for Insights Pillar

### Why This Matters

The insights pillar uses 88-level field metadata for data quality validation:
- **88-level fields** define allowed values for fields (e.g., `88 ACTIVE VALUE 'A'`)
- **Level-01 metadata records** define allowed values from metadata tables (e.g., `POLICY-TYPES` record)
- Both are used by `DataQualityValidationService` to validate parsed records

### Current Flow (Custom Adapter)

1. **Parse copybook** ‚Üí Extract 88-level fields ‚Üí Store in `validation_rules`
2. **Parse binary file** ‚Üí Create records
3. **Return result** ‚Üí Include `validation_rules` in metadata
4. **Insights pillar** ‚Üí Uses `validation_rules` to validate records

### Required Flow (Cobrix)

1. **Extract 88-level metadata** ‚Üí BEFORE cleaning copybook
2. **Clean copybook** ‚Üí Remove 88-level fields (Cobrix doesn't support them)
3. **Call Cobrix** ‚Üí Parse with cleaned copybook
4. **Return result** ‚Üí Include `validation_rules` in metadata (same format as custom adapter)
5. **Insights pillar** ‚Üí Uses `validation_rules` to validate records (same as custom adapter)

### Implementation Pattern

```python
# BEFORE cleaning (extract metadata)
validation_rules = extract_88_level_metadata(copybook_text)

# Clean copybook (removes 88-level fields)
cleaned_copybook = clean_copybook_for_cobrix(copybook_text)

# Parse with Cobrix
records = await parse_with_cobrix(file_data, cleaned_copybook)

# Return with validation_rules (for insights pillar)
return {
    "success": True,
    "records": records,
    "validation_rules": validation_rules  # Same format as custom adapter
}
```

### Integration with Insights Pillar

The insights pillar expects `validation_rules` in this format:
```python
{
    "88_level_fields": [
        {
            "field_name": "STATUS-CODE",
            "condition_name": "ACTIVE",
            "value": "A"
        }
    ],
    "metadata_records": [
        {
            "record_name": "POLICY-TYPES",
            "field_name": "TERM-LIFE",
            "value": "Term Life",
            "target_field": "POLICY-TYPE"
        }
    ]
}
```

This is the same format the custom adapter uses, ensuring consistency.

---

## üéØ Key Principles for Gold Standard Implementation

### 1. **Extract Metadata BEFORE Cleaning**
- Always extract 88-level fields BEFORE removing them from copybook
- Store in same format as custom adapter (for consistency)
- Include in response metadata for insights pillar

### 2. **Trust Cobrix**
- Cobrix is designed to handle COBOL variations
- Don't try to "fix" things Cobrix can handle
- Use Cobrix options, not workarounds

### 2. **Minimal Preprocessing**
- Only remove what Cobrix can't handle (88-level, VALUE clauses)
- Don't try to "improve" the copybook
- Let Cobrix parse it as-is

### 3. **Use Cobrix Options**
- `file_start_offset` - Don't modify the file
- `file_trailer_length` - Don't trim the file
- `encoding` - Let Cobrix detect or specify correctly
- `record_format` - Tell Cobrix the format

### 4. **No Workarounds**
- If parsing fails, fix the copybook or file format
- Don't add manual field extraction
- Don't retry with different strategies
- Fix the root cause

### 5. **Simple Error Handling**
- If Cobrix fails, return the error
- Don't try to "fix" the error with workarounds
- Let the user fix the copybook/file

---

## üìã Implementation Checklist

### Step 1: Extract 88-Level Metadata BEFORE Cleaning
- [ ] Extract 88-level fields BEFORE copybook cleaning
- [ ] Use existing `Level88Extractor` or `MainframeProcessingAdapter._extract_validation_rules()`
- [ ] Store in same format as custom adapter (for consistency)
- [ ] Include in response metadata for insights pillar

### Step 2: Simplify Copybook Cleaning
- [ ] Remove complex identifier stripping
- [ ] Remove field name replacements
- [ ] Remove PIC clause replacements
- [ ] Keep only: 88-level removal, VALUE removal, identifier removal, format conversion

### Step 3: Simplify File Format Detection
- [ ] Remove complex header detection
- [ ] Remove multiple retry strategies
- [ ] Use simple offset detection
- [ ] Pass offsets to Cobrix (don't modify file)

### Step 4: Remove Manual Field Fixes
- [ ] Remove all manual field extraction code
- [ ] Remove workarounds for AGE field
- [ ] Trust Cobrix to align fields correctly

### Step 5: Update Scala Code
- [ ] Add `file_start_offset` option
- [ ] Add `file_trailer_length` option
- [ ] Keep it simple

### Step 6: Test
- [ ] Verify 88-level metadata is extracted correctly
- [ ] Verify validation_rules are included in response
- [ ] Test insights pillar validation with 88-level rules
- [ ] Test with various copybook formats
- [ ] Test with various file formats
- [ ] Verify Cobrix parses correctly
- [ ] No 500 errors

---

## üéØ Expected Results

After implementing the gold standard approach:

1. **Reliability:** ‚úÖ Cobrix should parse files correctly
2. **Simplicity:** ‚úÖ Code is 200 lines vs 1400 lines
3. **Maintainability:** ‚úÖ Easy to understand and modify
4. **Performance:** ‚úÖ Faster (no multiple retries)
5. **Correctness:** ‚úÖ Cobrix handles all COBOL features automatically

---

## üìù Next Steps

1. **Review this analysis** - Confirm approach
2. **Implement simplified version** - Replace current complex code
3. **Test thoroughly** - Verify it works with various files
4. **Document** - Update documentation with gold standard approach

---

**Status:** Ready for implementation  
**Estimated Effort:** 4-6 hours to refactor  
**Priority:** High (needed for reliable Cobrix parsing)
